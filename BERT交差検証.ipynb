{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT交差検証.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1LvR5qVkcz4T_xgaV8lWRF18yxrCf0BXc",
      "authorship_tag": "ABX9TyNRVPxGAQeNs0mgyVZAfGRO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arumat546/nico_cls/blob/main/BERT%E4%BA%A4%E5%B7%AE%E6%A4%9C%E8%A8%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ejxdprn3cNBA",
        "outputId": "f50e8ce7-2280-4062-80e9-ddb5ad816aab"
      },
      "source": [
        "%cd /content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/5_15"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/5_15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSTdrUo2TCsQ"
      },
      "source": [
        "#複数のtsvを同一データフレームに格納\n",
        "import pandas as pd\n",
        "import glob\n",
        "from sklearn.model_selection import KFold\n",
        "import csv\n",
        "\n",
        "path = \"/content/drive/My Drive/Google Colab/niconico_dataset_add_smid/DAT  ASET/6mix\" # use your path\n",
        "all_files = glob.glob(path + \"/*.tsv\")#tsv\n",
        "#all_files = glob.glob(path + \"/*.csv\")#csv\n",
        " \n",
        "list_dataset = []\n",
        " \n",
        "for filename in all_files:\n",
        "    df2 = pd.read_table(filename, header=None)#tsv\n",
        "    #df2 = pd.read_csv(filename)#csv\n",
        "    list_dataset.append(df2)\n",
        " \n",
        "df = pd.concat(list_dataset, axis=0, ignore_index=True)\n",
        "#df = pd.read_table(\"/content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/5_15/8mix\",header=None)\n",
        "#データのシャッフル\n",
        "df = df.sample(frac=1, random_state=45).reset_index(drop=True)\n",
        "df.head()\n",
        "\n",
        "#交差検証(trainとtest)\n",
        "kf = KFold(4, shuffle=True)\n",
        "\n",
        "\n",
        "kfcount=1\n",
        "for train, test in kf.split(df):\n",
        "     train_df = df.iloc[train]\n",
        "     test_df = df.iloc[test]\n",
        "     print('(train)', train_df)\n",
        "     print('(test)', test_df)\n",
        "     print('-----')\n",
        "\n",
        "     if kfcount == 1:\n",
        "       test_df.to_csv(\"./tr1.tsv\", sep='\\t', index=False, header=None)\n",
        "       train_df.to_csv(\"./te1.tsv\", sep='\\t', index=False, header=None)\n",
        "     elif kfcount == 2:\n",
        "       test_df.to_csv(\"./te2.tsv\", sep='\\t', index=False, header=None)\n",
        "       train_df.to_csv(\"./tr2.tsv\", sep='\\t', index=False, header=None)\n",
        "     elif kfcount == 3:\n",
        "       test_df.to_csv(\"./te3.tsv\", sep='\\t', index=False, header=None)\n",
        "       train_df.to_csv(\"./tr3.tsv\", sep='\\t', index=False, header=None)\n",
        "     elif kfcount == 4:\n",
        "       test_df.to_csv(\"./te4.tsv\", sep='\\t', index=False, header=None)\n",
        "       train_df.to_csv(\"./tr4.tsv\", sep='\\t', index=False, header=None)\n",
        "     elif kfcount == 5:\n",
        "        test_df.to_csv(\"./te5.tsv\", sep='\\t', index=False, header=None)\n",
        "        train_df.to_csv(\"./tr5.tsv\", sep='\\t', index=False, header=None)\n",
        "     kfcount=kfcount+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrKz1OCQWrjV"
      },
      "source": [
        "# MeCabとtransformersの用意\n",
        "%%capture\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.996.5\n",
        "!pip install unidic-lite\n",
        "!pip install transformers==3.5.1\n",
        "!pip install fugashi\n",
        "!pip install ipadic\n",
        "!pip install torch==1.4.0\n",
        "!pip install torchtext==0.3.1\n",
        "import ipadic\n",
        "import torch\n",
        "import torchtext \n",
        "import torch.nn.functional as F\n",
        "from transformers.modeling_bert import BertModel\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "# 日本語BERTの分かち書き用tokenizer\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5Bws93caLWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb25c2ba-8953-4b66-8d54-3021e703613a"
      },
      "source": [
        "CTE=\"6カテゴリ\"\n",
        "trdata=\"tr1.tsv\"\n",
        "tedata=\"te1.tsv\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/4_19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcuujF5PS65x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "bd1d0974-9f5c-4a5e-9161-2cb3c9e8722e"
      },
      "source": [
        "!pip install -q gwpy\n",
        "\n",
        "# 乱数シードの固定\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED_VALUE = 1234  # これはなんでも良い\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.4MB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 17.9MB/s \n",
            "\u001b[?25h  Building wheel for ligo-segments (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ebd1fb148fe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED_VALUE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED_VALUE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED_VALUE\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# PyTorchを使う場合\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr8HQqXzWu24"
      },
      "source": [
        "%%capture\n",
        "CTE=\"8カテゴリ\"\n",
        "trdata=\"tr1.tsv\"\n",
        "tedata=\"te1.tsv\"\n",
        "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義\n",
        "max_length = 512  # 東北大学_日本語版の最大の単語数（サブワード数）は512\n",
        "\n",
        "def tokenizer_512(input_text):\n",
        "    \"\"\"torchtextのtokenizerとして扱えるように、512単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\n",
        "    return tokenizer.encode(input_text, max_length=512, return_tensors='pt')[0]\n",
        "\n",
        "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_512, use_vocab=False, lower=False, include_lengths=True, batch_first=True, fix_length=max_length, pad_token=0)\n",
        "# 注意：tokenize=tokenizer.encodeと、.encodeをつけます。padding[PAD]のindexが0なので、0を指定します。\n",
        "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# 各tsvファイルを読み込み、分かち書きをしてdatasetにします\n",
        "##########################################################################################################################################################################\n",
        "dataset_train_eval, dataset_test = torchtext.data.TabularDataset.splits(path='.', train=trdata, test=tedata, format='tsv', fields=[('content', TEXT), ('category', LABEL)])\n",
        "##########################################################################################################################################################################\n",
        "\n",
        "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
        "dataset_train, dataset_eval = dataset_train_eval.split(split_ratio=0.8, random_state=random.seed(1234))\n",
        "\n",
        "# datasetの長さを確認してみる\n",
        "print(dataset_train.__len__())\n",
        "print(dataset_eval.__len__())\n",
        "print(dataset_test.__len__())\n",
        "\n",
        "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
        "batch_size = 32  # BERTでは16、32あたりを使用する\n",
        "\n",
        "dl_train = torchtext.data.Iterator(\n",
        "    dataset_train, batch_size=batch_size, train=True)\n",
        "\n",
        "dl_eval = torchtext.data.Iterator(\n",
        "    dataset_eval, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "dl_test = torchtext.data.Iterator(\n",
        "    dataset_test, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": dl_train, \"val\": dl_eval}\n",
        "\n",
        "from torch import nn\n",
        "# BERTの日本語学習済みパラメータのモデル\n",
        "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "class BertForNico(nn.Module):\n",
        "    '''BERTモデルに3クラスを判定する部分をつなげたモデル'''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BertForNico, self).__init__()\n",
        "\n",
        "        # BERTモジュール\n",
        "        self.bert = model  # 日本語学習済みのBERTモデル\n",
        "\n",
        "        # headにクラス予測を追加\n",
        "        # 入力はBERTの出力特徴量の次元768、出力は3クラス\n",
        "        self.cls = nn.Linear(in_features=768, out_features=9)###############################################################################################クラス数注意！！！！！！！！！！！！！！！\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.cls.weight, std=0.02)\n",
        "        nn.init.normal_(self.cls.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        '''\n",
        "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
        "        '''\n",
        "\n",
        "        # BERTの基本モデル部分の順伝搬\n",
        "        # 順伝搬させる\n",
        "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
        "\n",
        "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
        "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
        "        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素\n",
        "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n",
        "        output = self.cls(vec_0)  # 全結合層\n",
        "\n",
        "        return output\n",
        "# モデル構築\n",
        "net = BertForNico()\n",
        "# 訓練モードに設定\n",
        "net.train()\n",
        "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
        "\n",
        "# 1. まず全部を、勾配計算Falseにしてしまう\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
        "for param in net.bert.encoder.layer[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "for param in net.cls.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 最適化手法の設定\n",
        "import torch.optim as optim\n",
        "\n",
        "# BERTの元の部分はファインチューニング\n",
        "optimizer = optim.Adam([\n",
        "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
        "    {'params': net.cls.parameters(), 'lr': 1e-4}\n",
        "])\n",
        "\n",
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n",
        "# 棄却有りのとき\n",
        "\n",
        "#正解率記録用リスト\n",
        "EpAc_train = []\n",
        "EpAc_val = []\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.content[0].to(device)  # 文章\n",
        "                labels = batch.category.to(device)  # ラベル\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BERTに入力\n",
        "                    outputs = net(inputs)\n",
        "   \n",
        "                    # 損失を計算(DG考慮)\n",
        "                    if epoch >= pretrain:\n",
        "                      outputs = F.softmax(outputs,dim=1)\n",
        "\n",
        "                      outputs, reservation = outputs[:,:-1], outputs[:,-1] #[全行、最後以外全列],[全行、最後の列]\n",
        "\n",
        "                    #unsqeeze(1)でonehotベクトル化\n",
        "                    #labelsの要素が1のとこと同じ要素をoutputsから取り出す\n",
        "                      gain = torch.gather(outputs,dim=1,index=labels.unsqueeze(1)).squeeze() \n",
        "\n",
        "                      doubling_rate = (gain.add(reservation.div(reward))).log()\n",
        "           \n",
        "                      loss = -doubling_rate.mean()\n",
        "\n",
        "                    else:\n",
        "                      loss = criterion(outputs[:,:-1],labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測　_は出力の最大値、predsは出力の最大位置（クラス）\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            acc = (torch.sum(preds == labels.data)\n",
        "                                   ).double()/batch_size\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n",
        "                                iteration, loss.item(),  acc))\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失と正解数の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            #正解率をリストに格納\n",
        "            if phase == 'train':\n",
        "              EpAc_train.append(epoch_acc)\n",
        "            \n",
        "            else :\n",
        "              EpAc_val.append(epoch_acc)\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "    return net"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfBJQQO1bWbd"
      },
      "source": [
        "## **検証**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBfsTYaMZdy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de3425a0-53ce-4e89-e8b2-e88dc11e0366"
      },
      "source": [
        "# 学習・検証 139minutes\n",
        "num_epochs = 9\n",
        "pretrain = 3\n",
        "reward =  7.7  #棄却度を決めるハイパーパラメータ\n",
        "\n",
        "net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)#10epochで2.8時間"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用デバイス： cuda:0\n",
            "-----start-------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXDYMrC3kB0_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hHoDgkIo9jS"
      },
      "source": [
        "ここまで\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT7f5fkXjEti",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "91c0f8db-ba1f-4cb0-adba-ef0247922364"
      },
      "source": [
        "#グラフを描画\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "\n",
        "epoch = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
        "#EpAc_train = [0.60, 0.65, 0.66, 0.70, 0.68]\n",
        "#EpAc_val = [0.63, 0.67, 0.68, 0.72, 0.68]\n",
        "\n",
        "plt.title(\"transition\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.plot(epoch, EpAc_train, label = \"train\")\n",
        "plt.plot(epoch, EpAc_val, label = \"val\")\n",
        "plt.legend(loc = \"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIHBMZ-KcenK"
      },
      "source": [
        "#自作テストデータで検証する場合\n",
        "%%capture\n",
        "%cd /content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/TestData/動物\n",
        "tedata=\"カワウソ.tsv\"\n",
        "dataset_train_eval, dataset_test = torchtext.data.TabularDataset.splits(path='.', train='/content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/TestData/動物/damy.tsv', test=tedata, format='tsv', fields=[('content', TEXT), ('category', LABEL)])\n",
        "dl_test = torchtext.data.Iterator(dataset_test, batch_size=batch_size, train=False, sort=False)\n",
        "batch = next(iter(dl_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6FQ_sAibJzz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5be102-029a-45aa-d1b5-731b6f7811a4"
      },
      "source": [
        "#提案手法\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "expected_coverage = [90,80,70,60,50]\n",
        "text_data=[]\n",
        "abstention_results = []\n",
        "preds_all = np.array([],dtype=int)\n",
        "TEX=[]\n",
        "ABST=[]\n",
        "LAB=[]\n",
        "EXPORT=[]\n",
        "SORT=[]\n",
        "acc_cov=[]\n",
        "abscount=0\n",
        "\n",
        "# テストデータでの正解率を求める\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "# epochの正解数を記録する変数\n",
        "epoch_corrects = 0\n",
        "# for batch in tqdm(dl_test): \n",
        "for batch in dl_test:  # testデータのDataLoader\n",
        "    # batchはTextとLableの辞書オブジェクト\n",
        "    # GPUが使えるならGPUにデータを送る\n",
        "    inputs = batch.content[0].to(device)  # 文章\n",
        "    labels = batch.category.to(device)  # ラベル\n",
        "    #print(batch.content[0])\n",
        "\n",
        "    # 順伝搬（forward）計算\n",
        "    with torch.set_grad_enabled(False):\n",
        "        # BertForNicoに入力\n",
        "      outputs = net_trained(inputs)\n",
        "      values, predictions = torch.max(outputs, 1) \n",
        "      for j in range(len(predictions)):\n",
        "        if predictions[j].cpu().numpy() == 6:\n",
        "          abscount += 1\n",
        "      \n",
        "    #損失計算\n",
        "      outputs = F.softmax(outputs,dim=1)\n",
        "      outputs, reservation = outputs[:,:-1], outputs[:,-1] #[全行、最後以外全列],[全行、最後の列]\n",
        "\n",
        "      abstention_results.extend(zip(list(reservation.cpu().numpy()),list(predictions.eq(labels.data).cpu().numpy()),list(inputs.data.cpu().numpy())))\n",
        "      gain = torch.gather(outputs,dim=1,index=labels.unsqueeze(1)).squeeze()\n",
        "      doubling_rate = (gain.add(reservation.div(reward))).log()\n",
        "\n",
        "      loss = -doubling_rate.mean()\n",
        "      preds_all = np.append(preds_all, predictions.cpu().numpy())\n",
        "      epoch_corrects += torch.sum(predictions == labels.data)  # 正解数の合計を更新\n",
        "\n",
        "#結果をファイル出力\n",
        "df = pd.read_table(tedata,header=None)\n",
        "for i in range(len(dl_test.dataset)):\n",
        "  TEX.append(df.iloc[i,0])\n",
        "  LAB.append(df.iloc[i,1])\n",
        "  ABST.append(abstention_results[i][0])#棄却値をリストに格納\n",
        "EXPORT.extend(zip(TEX,LAB,ABST))\n",
        "#ソート\n",
        "SORT = sorted(EXPORT, reverse=False, key=lambda x: x[2])\n",
        "#出力\n",
        "with open(\"result_n1.tsv\", \"w\") as f:\n",
        "  writer = csv.writer(f, lineterminator='\\n',delimiter='\\t')\n",
        "  writer.writerows(SORT)\n",
        "\n",
        "#棄却判定後の計算\n",
        "#棄権の結果を予約の高低に応じて並べ替える(降順)\n",
        "abstention_results.sort(key = lambda x: x[0], reverse=True)#リストの1列目の要素をキーに並び替える\n",
        "\n",
        "# ソートされた結果の「正しいかどうか」のリストを取得する\n",
        "#map(関数、加工元)…加工元データに関数をかけたものをリスト取得する\n",
        "sorted_correct = list(map(lambda x: int(x[1]), abstention_results))\n",
        "\n",
        "\n",
        "#print(\"sorted_correct\")\n",
        "#print(sorted_correct)\n",
        "\n",
        "size = len(dl_test.dataset)\n",
        "i=0\n",
        "print('accracy of coverage ',end='')\n",
        "for coverage in expected_coverage:\n",
        "  #print('{:.0f}: {:.3f}, '.format(coverage, sum(sorted_correct[int(size*coverage/100):])),end='')\n",
        "  print(coverage)\n",
        "  count1=int(size*(100-coverage)/100) \n",
        "  count2=sum(sorted_correct[count1:])\n",
        "  count3=size-count1 #カバレッジ考慮のデータ数\n",
        "  acc_cov.append(count2/count3)\n",
        "  \n",
        "  print(acc_cov[i])\n",
        "  i+=1\n",
        "  #棄却されたテキストの表示\n",
        "  #if coverage==90:\n",
        "    #for j in range(count1):  \n",
        "      #print(tokenizer.convert_ids_to_tokens(text_data[j]))\n",
        "\n",
        "# 正解率\n",
        "epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
        "print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_acc))\n",
        "\n",
        "epoch_abs_acc=epoch_corrects.double() / (len(dl_test.dataset)-abscount)\n",
        "print(abscount)\n",
        "#print(preds_all)\n",
        "print(epoch_abs_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accracy of coverage 90\n",
            "0.33447098976109213\n",
            "80\n",
            "0.3769230769230769\n",
            "70\n",
            "0.4298245614035088\n",
            "60\n",
            "0.5025641025641026\n",
            "50\n",
            "0.6012269938650306\n",
            "テストデータ325個での正解率：0.3015\n",
            "208\n",
            "tensor(0.8376, device='cuda:0', dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7FSaOlhSyKu"
      },
      "source": [
        "kokokara\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQJe1z6cY8_o",
        "outputId": "0d27d6df-067d-489f-b2c3-b5676a01b35f"
      },
      "source": [
        "print(cm_normalized)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.01 0.04 0.04 0.   0.07 0.02 0.82]\n",
            " [ nan  nan  nan  nan  nan  nan  nan]\n",
            " [ nan  nan  nan  nan  nan  nan  nan]\n",
            " [ nan  nan  nan  nan  nan  nan  nan]\n",
            " [ nan  nan  nan  nan  nan  nan  nan]\n",
            " [ nan  nan  nan  nan  nan  nan  nan]\n",
            " [ nan  nan  nan  nan  nan  nan  nan]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WR6ILYDJEU9"
      },
      "source": [
        "#実際の動画での検証\n",
        "%%capture\n",
        "%cd /content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/TestData/字数無\n",
        "ca=0\n",
        "CTE=\"ゲーム(前無)\"\n",
        "tedata=\"g3.tsv\"\n",
        "dataset_train_eval, dataset_test = torchtext.data.TabularDataset.splits(path='.', train='/content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/TestData/動物/damy.tsv', test=tedata, format='tsv', fields=[('content', TEXT), ('category', LABEL)])\n",
        "dl_test = torchtext.data.Iterator(dataset_test, batch_size=batch_size, train=False, sort=False)\n",
        "batch = next(iter(dl_test))\n",
        "\n",
        "#提案手法\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "expected_coverage = [90,80,70,60,50]\n",
        "text_data=[]\n",
        "abstention_results = []\n",
        "preds_all = np.array([],dtype=int)\n",
        "TEX=[]\n",
        "ABST=[]\n",
        "LAB=[]\n",
        "EXPORT=[]\n",
        "SORT=[]\n",
        "acc_cov=[]\n",
        "abscount=0\n",
        "\n",
        "# テストデータでの正解率を求める\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "# epochの正解数を記録する変数\n",
        "epoch_corrects = 0\n",
        "# for batch in tqdm(dl_test): \n",
        "for batch in dl_test:  # testデータのDataLoader\n",
        "    # batchはTextとLableの辞書オブジェクト\n",
        "    # GPUが使えるならGPUにデータを送る\n",
        "    inputs = batch.content[0].to(device)  # 文章\n",
        "    labels = batch.category.to(device)  # ラベル\n",
        "    #print(batch.content[0])\n",
        "\n",
        "    # 順伝搬（forward）計算\n",
        "    with torch.set_grad_enabled(False):\n",
        "        # BertForNicoに入力\n",
        "      outputs = net_trained(inputs)\n",
        "      values, predictions = torch.max(outputs, 1) \n",
        "      for j in range(len(predictions)):\n",
        "        if predictions[j].cpu().numpy() == 6:\n",
        "          abscount += 1\n",
        "      \n",
        "    #損失計算\n",
        "      outputs = F.softmax(outputs,dim=1)\n",
        "      outputs, reservation = outputs[:,:-1], outputs[:,-1] #[全行、最後以外全列],[全行、最後の列]\n",
        "\n",
        "      abstention_results.extend(zip(list(reservation.cpu().numpy()),list(predictions.eq(labels.data).cpu().numpy()),list(inputs.data.cpu().numpy())))\n",
        "      gain = torch.gather(outputs,dim=1,index=labels.unsqueeze(1)).squeeze()\n",
        "      doubling_rate = (gain.add(reservation.div(reward))).log()\n",
        "\n",
        "      loss = -doubling_rate.mean()\n",
        "      preds_all = np.append(preds_all, predictions.cpu().numpy())\n",
        "      epoch_corrects += torch.sum(predictions == labels.data)  # 正解数の合計を更新\n",
        "\n",
        "#結果をファイル出力\n",
        "df = pd.read_table(tedata,header=None)\n",
        "for i in range(len(dl_test.dataset)):\n",
        "  TEX.append(df.iloc[i,0])\n",
        "  LAB.append(df.iloc[i,1])\n",
        "  ABST.append(abstention_results[i][0])#棄却値をリストに格納\n",
        "EXPORT.extend(zip(TEX,LAB,ABST))\n",
        "#ソート\n",
        "SORT = sorted(EXPORT, reverse=False, key=lambda x: x[2])\n",
        "#出力\n",
        "\n",
        "\n",
        "#棄却判定後の計算\n",
        "#棄権の結果を予約の高低に応じて並べ替える(降順)\n",
        "abstention_results.sort(key = lambda x: x[0], reverse=True)#リストの1列目の要素をキーに並び替える\n",
        "\n",
        "# ソートされた結果の「正しいかどうか」のリストを取得する\n",
        "#map(関数、加工元)…加工元データに関数をかけたものをリスト取得する\n",
        "sorted_correct = list(map(lambda x: int(x[1]), abstention_results))\n",
        "\n",
        "\n",
        "#print(\"sorted_correct\")\n",
        "#print(sorted_correct)\n",
        "\n",
        "size = len(dl_test.dataset)\n",
        "i=0\n",
        "print('accracy of coverage ',end='')\n",
        "for coverage in expected_coverage:\n",
        "  #print('{:.0f}: {:.3f}, '.format(coverage, sum(sorted_correct[int(size*coverage/100):])),end='')\n",
        "  print(coverage)\n",
        "  count1=int(size*(100-coverage)/100) \n",
        "  count2=sum(sorted_correct[count1:])\n",
        "  count3=size-count1 #カバレッジ考慮のデータ数\n",
        "  acc_cov.append(count2/count3)\n",
        "  \n",
        "  print(acc_cov[i])\n",
        "  i+=1\n",
        "  #棄却されたテキストの表示\n",
        "  #if coverage==90:\n",
        "    #for j in range(count1):  \n",
        "      #print(tokenizer.convert_ids_to_tokens(text_data[j]))\n",
        "\n",
        "# 正解率\n",
        "epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
        "print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_acc))\n",
        "\n",
        "epoch_abs_acc=epoch_corrects.double() / (len(dl_test.dataset)-abscount)\n",
        "print(abscount)\n",
        "#print(preds_all)\n",
        "print(epoch_abs_acc)\n",
        "\n",
        "#推論結果の出力\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "df = pd.Cov = pd.read_csv(tedata, sep='\\t',names=[\"content\", \"category\", \"predict\"])\n",
        "net_trained.eval()  #推論モードに\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    df.at[index, \"predict\"] = preds_all[index]  # GPU環境の場合は「.cpu().numpy()」としてください。\n",
        "\n",
        "df.to_csv(\"predicted.tsv\", sep='\\t', index=False)\n",
        "#混同行列の表示（評価）\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "#有効数字\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "y_true =[]\n",
        "y_pred =[]\n",
        "df = pd.read_csv(\"predicted.tsv\",  sep='\\t',engine=\"python\", encoding=\"utf-8\")\n",
        "for index, row in df.iterrows():\n",
        "    if row['category'] == 0:\n",
        "        y_true.append(\"POLITICS\")\n",
        "    if row['category'] ==1:\n",
        "        y_true.append(\"COOK\")\n",
        "    if row['category'] ==2:\n",
        "        y_true.append(\"ANIMAL\")\n",
        "    if row['category'] ==3:\n",
        "        y_true.append(\"TRAIN\")\n",
        "    if row['category'] ==4:\n",
        "        y_true.append(\"SPORT\")\n",
        "    if row['category'] ==5:\n",
        "        y_true.append(\"GAME\")    \n",
        "    if row['category'] ==6:\n",
        "        y_true.append(\"ABSTENTION\")     \n",
        "    if row['predict'] ==0.0:\n",
        "        y_pred.append(\"POLITICS\")\n",
        "    if row['predict'] ==1.0:\n",
        "        y_pred.append(\"COOK\")\n",
        "    if row['predict'] ==2.0:\n",
        "        y_pred.append(\"ANIMAL\")\n",
        "    if row['predict'] ==3.0:\n",
        "        y_pred.append(\"TRAIN\")\n",
        "    if row['predict'] ==4.0:\n",
        "        y_pred.append(\"SPORT\")\n",
        "    if row['predict'] ==5.0:\n",
        "        y_pred.append(\"GAME\")\n",
        "    if row['predict'] ==6.0:\n",
        "        y_pred.append(\"ABSTENTION\")\n",
        "\n",
        "\n",
        "print(len(y_true))\n",
        "print(len(y_pred))\n",
        "\n",
        "\n",
        "# 混同行列(confusion matrix)の取得\n",
        "labels = [\"POLITICS\", \"COOK\", \"ANIMAL\",\"TRAIN\",\"SPORT\",\"GAME\",\"ABSTENTION\"]\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# # データフレームに変換\n",
        "# cm_labeled = pd.DataFrame(cm_normalized, columns=labels, index=labels)\n",
        "\n",
        "# # 結果の表示\n",
        "# #cm_labeled\n",
        "# plt.figure(figsize=(15, 15))\n",
        "# sns.set(font_scale=2.5,font=\"Times New Roman\") \n",
        "# sns.set_style(\"darkgrid\", {'font.family':'serif', 'font.serif':'Times New Roman'})\n",
        "# sns.heatmap(cm_labeled,annot=True,cmap='Blues', fmt=\"1.2f\",square=True)\n",
        "# plt.xlabel('Predicted Label', fontsize = 35) # x-axis label with fontsize 15\n",
        "# plt.ylabel('True Label', fontsize = 35) # y-axis label with fontsize 15\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "print(cm_normalized)\n",
        "#tsvに出力\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/realvideo.tsv\",\"a\") as f:\n",
        "  writer = csv.writer(f,delimiter='\\t')\n",
        "  \n",
        "  writer.writerow([CTE,tedata,len(dl_test.dataset),abscount,epoch_abs_acc.cpu().numpy(),cm_normalized[ca][0],cm_normalized[ca][1],cm_normalized[ca][2],cm_normalized[ca][3],cm_normalized[ca][4],cm_normalized[ca][5],cm_normalized[ca][6]])\n",
        "\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01tS5Z5ISqFc"
      },
      "source": [
        "kokomade\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng5tNeXJz2MN",
        "outputId": "d27f22b6-0598-40c9-aff7-242679c9e9c6"
      },
      "source": [
        "#推論結果の出力\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "df = pd.Cov = pd.read_csv(tedata, sep='\\t',names=[\"content\", \"category\", \"predict\"])\n",
        "net_trained.eval()  #推論モードに\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    df.at[index, \"predict\"] = preds_all[index]  # GPU環境の場合は「.cpu().numpy()」としてください。\n",
        "\n",
        "df.to_csv(\"predicted.tsv\", sep='\\t', index=False)\n",
        "#混同行列の表示（評価）\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "#有効数字\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "y_true =[]\n",
        "y_pred =[]\n",
        "df = pd.read_csv(\"predicted.tsv\",  sep='\\t',engine=\"python\", encoding=\"utf-8\")\n",
        "for index, row in df.iterrows():\n",
        "    if row['category'] == 0:\n",
        "        y_true.append(\"POLITICS\")\n",
        "    if row['category'] ==1:\n",
        "        y_true.append(\"COOK\")\n",
        "    if row['category'] ==2:\n",
        "        y_true.append(\"ANIMAL\")\n",
        "    if row['category'] ==3:\n",
        "        y_true.append(\"TRAIN\")\n",
        "    if row['category'] ==4:\n",
        "        y_true.append(\"SPORT\")\n",
        "    if row['category'] ==5:\n",
        "        y_true.append(\"GAME\")    \n",
        "    if row['category'] ==6:\n",
        "        y_true.append(\"ABSTENTION\")     \n",
        "    if row['predict'] ==0.0:\n",
        "        y_pred.append(\"POLITICS\")\n",
        "    if row['predict'] ==1.0:\n",
        "        y_pred.append(\"COOK\")\n",
        "    if row['predict'] ==2.0:\n",
        "        y_pred.append(\"ANIMAL\")\n",
        "    if row['predict'] ==3.0:\n",
        "        y_pred.append(\"TRAIN\")\n",
        "    if row['predict'] ==4.0:\n",
        "        y_pred.append(\"SPORT\")\n",
        "    if row['predict'] ==5.0:\n",
        "        y_pred.append(\"GAME\")\n",
        "    if row['predict'] ==6.0:\n",
        "        y_pred.append(\"ABSTENTION\")\n",
        "\n",
        "\n",
        "print(len(y_true))\n",
        "print(len(y_pred))\n",
        "\n",
        "\n",
        "# 混同行列(confusion matrix)の取得\n",
        "labels = [\"POLITICS\", \"COOK\", \"ANIMAL\",\"TRAIN\",\"SPORT\",\"GAME\",\"ABSTENTION\"]\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# データフレームに変換\n",
        "cm_labeled = pd.DataFrame(cm_normalized, columns=labels, index=labels)\n",
        "\n",
        "# 結果の表示\n",
        "#cm_labeled\n",
        "plt.figure(figsize=(15, 15))\n",
        "sns.set(font_scale=2.5,font=\"Times New Roman\") \n",
        "sns.set_style(\"darkgrid\", {'font.family':'serif', 'font.serif':'Times New Roman'})\n",
        "sns.heatmap(cm_labeled,annot=True,cmap='Blues', fmt=\"1.2f\",square=True)\n",
        "plt.xlabel('Predicted Label', fontsize = 35) # x-axis label with fontsize 15\n",
        "plt.ylabel('True Label', fontsize = 35) # y-axis label with fontsize 15\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "931\n",
            "931\n",
            "[[0.89 0.01 0.01 0.01 0.02 0.   0.07]\n",
            " [ nan  nan  nan  nan  nan  nan  nan]\n",
            " [ nan  nan  nan  nan  nan  nan  nan]\n",
            " [ nan  nan  nan  nan  nan  nan  nan]\n",
            " [ nan  nan  nan  nan  nan  nan  nan]\n",
            " [ nan  nan  nan  nan  nan  nan  nan]\n",
            " [ nan  nan  nan  nan  nan  nan  nan]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6utzDzyEICr7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5b72bc3-b539-4280-f126-6dbbfcf16ff6"
      },
      "source": [
        "#各種推論計算結果の表示\n",
        "y_true =[]\n",
        "y_pred =[]\n",
        "\n",
        "df = pd.read_csv(\"predicted.tsv\", sep='\\t', engine=\"python\", encoding=\"utf-8\")\n",
        "for index, row in df.iterrows():\n",
        "    y_true.append(row[\"category\"])\n",
        "    y_pred.append(row[\"predict\"])\n",
        "\n",
        "_pred = [\n",
        "    round(float(val)) for val in y_pred\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "ALL=5000#全データ数\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TG_ALL=0#対象カテゴリ全データ数\n",
        "TG_ABS=0#対象カテゴリの棄権データ数\n",
        "PR_ALL=0#対象カテゴリを推測したデータ数\n",
        "TP=0#対象カテゴリの推測が正解したデータ数\n",
        "\n",
        "#ラベル0 : 対象カテゴリクラス\n",
        "#ラベル2 : 棄権クラス\n",
        "for i in range(len(y_true)):\n",
        "  if y_true[i]==0:\n",
        "    TG_ALL+=1\n",
        "  if y_pred[i]==6 and y_true[i]==0:\n",
        "    TG_ABS+=1\n",
        "  if y_pred[i]==0 and y_true[i]==0:\n",
        "    TP+=1\n",
        "  if y_pred[i]==0 :\n",
        "    PR_ALL+=1\n",
        "\n",
        "RECALL=TP/(TG_ALL-TG_ABS)\n",
        "PRECISION=TP/PR_ALL\n",
        "F1=2*RECALL*PRECISION/(RECALL+PRECISION)\n",
        "\n",
        "\n",
        "\n",
        "print(\"棄却数：{}\".format(abscount))\n",
        "absdiv=len(dl_test.dataset)-abscount\n",
        "ACCURACY=epoch_corrects.double() /absdiv\n",
        "print(\"Accuracy(正解率)：{}\".format(ACCURACY))   \n",
        "\n",
        "print(\"Recall(再現率)：{}\".format(RECALL))\n",
        "print(\"Precision(適合率)：{}\".format(PRECISION))\n",
        "print(\"F1score：{}\".format(F1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "棄却数：8074\n",
            "Accuracy(正解率)：0.7597458850707479\n",
            "Recall(再現率)：0.8329637841832964\n",
            "Precision(適合率)：0.8119596541786743\n",
            "F1score：0.8223276176577893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RewmmhUX5ViE"
      },
      "source": [
        "#tsvに出力\n",
        "import csv\n",
        "%cd /content/drive/MyDrive/Google Colab/niconico_dataset_add_smid\n",
        "with open(\"/content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/TestResult.tsv\",\"a\") as f:\n",
        "  writer = csv.writer(f,delimiter='\\t')\n",
        "  \n",
        "  writer.writerow([CTE,trdata,reward,num_epochs,EpAc_train[-1].cpu().numpy(),EpAc_val[-1].cpu().numpy(),abscount,ACCURACY.cpu().numpy(),RECALL,PRECISION,F1,epoch_acc.cpu().numpy(),acc_cov[0],acc_cov[1],acc_cov[2],acc_cov[3],acc_cov[4]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw-QbdgEbSxV"
      },
      "source": [
        "## **描画**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogkHfaPBVyjd"
      },
      "source": [
        "#棄却考慮した新たなデータセット作成\n",
        "df = pd.read_csv(\"predicted_n.tsv\",  sep='\\t',engine=\"python\", encoding=\"utf-8\")\n",
        "\n",
        "df_org = df.copy()\n",
        "\n",
        "# for index, row in df.iterrows():\n",
        "#   df_org.drop(row['predict']=6.0, inplace=True)\n",
        "df_org=df[df.predict != 6.0]\n",
        "df2=df_org.drop('predict', axis=1)\n",
        "\n",
        "df2.to_csv(\"tr_n.tsv\", sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJglx33F22jO"
      },
      "source": [
        "#グラフを描画\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from matplotlib import rcParams\n",
        "\n",
        "# epoch = [\"1\", \"2\", \"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
        "# #EpAc_train = [0.60, 0.65, 0.66, 0.70, 0.68]\n",
        "# #EpAc_val = [0.63, 0.67, 0.68, 0.72, 0.68]\n",
        "\n",
        "# plt.title(\"transition\")\n",
        "# plt.xlabel(\"epoch\")\n",
        "# plt.ylabel(\"accuracy\")\n",
        "# plt.plot(epoch, EpAc_train, label = \"train\")\n",
        "# plt.plot(epoch, EpAc_val, label = \"val\")\n",
        "# plt.legend(loc = \"upper left\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-35d8iaQSJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47eebc5a-051b-4b93-fbf0-74a8052efad9"
      },
      "source": [
        "print(EpAc_train)\n",
        "print(EpAc_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor(0.1622, device='cuda:0', dtype=torch.float64), tensor(0.3888, device='cuda:0', dtype=torch.float64), tensor(0.4067, device='cuda:0', dtype=torch.float64), tensor(0.4566, device='cuda:0', dtype=torch.float64), tensor(0.4606, device='cuda:0', dtype=torch.float64), tensor(0.4714, device='cuda:0', dtype=torch.float64), tensor(0.4758, device='cuda:0', dtype=torch.float64), tensor(0.4845, device='cuda:0', dtype=torch.float64), tensor(0.4848, device='cuda:0', dtype=torch.float64)]\n",
            "[tensor(0.4171, device='cuda:0', dtype=torch.float64), tensor(0.4488, device='cuda:0', dtype=torch.float64), tensor(0.4434, device='cuda:0', dtype=torch.float64), tensor(0.4748, device='cuda:0', dtype=torch.float64), tensor(0.4890, device='cuda:0', dtype=torch.float64), tensor(0.4884, device='cuda:0', dtype=torch.float64), tensor(0.5061, device='cuda:0', dtype=torch.float64), tensor(0.4927, device='cuda:0', dtype=torch.float64), tensor(0.5106, device='cuda:0', dtype=torch.float64)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jrle04Il9-_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12cd0591-180a-492a-c620-2ab14556efd0"
      },
      "source": [
        "# # #保存済みモデルを使う場合\n",
        "%cd /content/drive/MyDrive/Google Colab/niconico_dataset_add_smid\n",
        "net_trained.load_state_dict(torch.load(\"model_bert5.7_all.pth\"))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Google Colab/niconico_dataset_add_smid\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP3O-YWPbNNp"
      },
      "source": [
        "## **テスト**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGW0XikapP_y"
      },
      "source": [
        "#棄却度順に文章表示\n",
        "#for j in range(count1):  \n",
        " # print(tokenizer.convert_ids_to_tokens(text_data[j]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOanX9vWJd8V"
      },
      "source": [
        "# #ABS考慮した評価指標\n",
        "# print(y_true)\n",
        "# print(y_pred)\n",
        "# y_pred = [\n",
        "#     round(float(val)) for val in y_pred\n",
        "# ]\n",
        "\n",
        "# TP=0\n",
        "\n",
        "# ALL=5000\n",
        "# TG_ALL=0\n",
        "# TG_ABS=0\n",
        "# PR_ALL=0\n",
        "\n",
        "# for i in range(len(y_true)):\n",
        "#   if y_true[i]==0:\n",
        "#     TG_ALL+=1\n",
        "#   if y_pred[i]==2 and y_true[i]==0:\n",
        "#     TG_ABS+=1\n",
        "#   if y_pred[i]==0 and y_true[i]==0:\n",
        "#     TP+=1\n",
        "#   if y_pred[i]==0 :\n",
        "#     PR_ALL+=1\n",
        "\n",
        "# RECALL=TP/(TG_ALL-TG_ABS)\n",
        "# PRECISION=TP/PR_ALL\n",
        "\n",
        "# print(\"Recall(再現率)：{}\".format(RECALL))\n",
        "# #print(TP/(TG_ALL-TG_ABS))\n",
        "# print(\"Precision(適合率)：{}\".format(PRECISION))\n",
        "# #print(TP/PR_ALL)\n",
        "# print(\"F1score：\")\n",
        "# print(2*RECALL*PRECISION/(RECALL+PRECISION))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7DSWs5lVDhP"
      },
      "source": [
        "#推論結果の出力\n",
        "# %cd /content/drive/My Drive/Google Colab/niconico_dataset_add_smid\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.metrics import precision_score\n",
        "# from sklearn.metrics import recall_score\n",
        "# from sklearn.metrics import f1_score\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "# df = pd.Cov = pd.read_csv(tedata, sep='\\t',names=[\"content\", \"category\", \"predict\"])\n",
        "# net_trained.eval()  #推論モードに\n",
        "\n",
        "# for index, row in df.iterrows():\n",
        "#     df.at[index, \"predict\"] = preds_all[index]  # GPU環境の場合は「.cpu().numpy()」としてください。\n",
        "\n",
        "# df.to_csv(\"predicted.tsv\", sep='\\t', index=False)\n",
        "# #混同行列の表示（評価）\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# import seaborn as sns\n",
        "\n",
        "# y_true =[]\n",
        "# y_pred =[]\n",
        "# df = pd.read_csv(\"predicted.tsv\",  sep='\\t',engine=\"python\", encoding=\"utf-8\")\n",
        "# for index, row in df.iterrows():\n",
        "#     if row['category'] == 0:\n",
        "#         y_true.append(\"POLITICS\")\n",
        "#     if row['category'] ==1:\n",
        "#         y_true.append(\"COOKING\")\n",
        "#     if row['category'] ==2:\n",
        "#         y_true.append(\"ANIMAL\")\n",
        "#     if row['category'] ==3:\n",
        "#         y_true.append(\"ABSTENTION\")\n",
        "#     if row['predict'] ==0.0:\n",
        "#         y_pred.append(\"POLITICS\")\n",
        "#     if row['predict'] ==1.0:\n",
        "#         y_pred.append(\"COOKING\")\n",
        "#     if row['predict'] ==2.0:\n",
        "#         y_pred.append(\"ANIMAL\")\n",
        "#     if row['predict'] ==3.0:\n",
        "#         y_pred.append(\"ABSTENTION\")\n",
        "\n",
        "# print(len(y_true))\n",
        "# print(len(y_pred))\n",
        "\n",
        "# # 混同行列(confusion matrix)の取得\n",
        "# labels = [\"POLITICS\", \"COOKING\", \"ANIMAL\", \"ABSTENTION\"]\n",
        "# cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "# cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# # データフレームに変換\n",
        "# cm_labeled = pd.DataFrame(cm_normalized, columns=labels, index=labels)\n",
        "\n",
        "# # 結果の表示\n",
        "# #cm_labeled\n",
        "# sns.set(font_scale=0.7) \n",
        "# sns.heatmap(cm_labeled,annot=True,cmap='Blues',fmt='.2g',square=True)\n",
        "# plt.xlabel('Predicted Label', fontsize = 9) # x-axis label with fontsize 15\n",
        "# plt.ylabel('True Label', fontsize = 9) # y-axis label with fontsize 15\n",
        "\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDcDtCzMlvqi"
      },
      "source": [
        "# #混同分布からテキスト表示（政治）\n",
        "# s_c =[]\n",
        "# s_p =[]\n",
        "# s_ab =[]\n",
        "# df = pd.read_csv(\"predicted.tsv\",  sep='\\t',engine=\"python\", encoding=\"utf-8\")\n",
        "# for index, row in df.iterrows():\n",
        "#     if row['category'] == 0 and row['predict'] == 1:\n",
        "#         s_c.append(row['content'])\n",
        "#     if row['category'] == 0 and row['predict'] == 2:\n",
        "#         s_p.append(row['content'])\n",
        "#     if row['category'] == 0 and row['predict'] == 3:   \n",
        "#         s_ab.append(row['content'])   \n",
        "\n",
        "# with open('/content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/TestDate/text/安倍.txt', 'w') as f:\n",
        "#     f.write(\"[政治/料理]################################################################\\n\")\n",
        "#     for d in s_c:\n",
        "#         f.write(\"%s\\n\" % d)\n",
        "#     f.write(\"\\n\\n[政治/動物]############################################################\\n\")\n",
        "#     for d in s_p:\n",
        "#         f.write(\"%s\\n\" % d)\n",
        "#     f.write(\"\\n\\n[政治/棄却]############################################################\\n\")\n",
        "#     for d in s_ab:\n",
        "#         f.write(\"%s\\n\" % d)                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujtZu0olzYoy"
      },
      "source": [
        "# #混同分布からテキスト表示（料理）\n",
        "# s_c =[]\n",
        "# s_p =[]\n",
        "# s_ab =[]\n",
        "# df = pd.read_csv(\"predicted.tsv\",  sep='\\t',engine=\"python\", encoding=\"utf-8\")\n",
        "# for index, row in df.iterrows():\n",
        "#     if row['category'] == 1 and row['predict'] == 0:\n",
        "#         s_c.append(row['content'])\n",
        "#     if row['category'] == 1 and row['predict'] == 2:\n",
        "#         s_p.append(row['content'])\n",
        "#     if row['category'] == 1 and row['predict'] == 3:   \n",
        "#         s_ab.append(row['content'])   \n",
        "\n",
        "# with open('/content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/TestDate/text/菜の花.txt', 'w') as f:\n",
        "#     f.write(\"[料理/政治]###############################################################################\\n\")\n",
        "#     for d in s_c:\n",
        "#         f.write(\"%s\\n\" % d)\n",
        "#     f.write(\"\\n\\n[料理/動物]###############################################################################\\n\")\n",
        "#     for d in s_p:\n",
        "#         f.write(\"%s\\n\" % d)\n",
        "#     f.write(\"\\n\\n[料理/棄却]###############################################################################\\n\")\n",
        "#     for d in s_ab:\n",
        "#         f.write(\"%s\\n\" % d)                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iXs0shoyrTc"
      },
      "source": [
        "# #混同分布からテキスト表示（動物）\n",
        "# s_c =[]\n",
        "# s_p =[]\n",
        "# s_ab =[]\n",
        "# df = pd.read_csv(\"predicted.tsv\",  sep='\\t',engine=\"python\", encoding=\"utf-8\")\n",
        "# for index, row in df.iterrows():\n",
        "#     if row['category'] == 2 and row['predict'] == 0:\n",
        "#         s_c.append(row['content'])\n",
        "#     if row['category'] == 2 and row['predict'] == 1:\n",
        "#         s_p.append(row['content'])\n",
        "#     if row['category'] == 2 and row['predict'] == 3:   \n",
        "#         s_ab.append(row['content'])   \n",
        "\n",
        "# with open('/content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/TestDate/text/犬食べた.txt', 'w') as f:\n",
        "#     f.write(\"[動物/政治]###############################################################################\\n\")\n",
        "#     for d in s_c:\n",
        "#         f.write(\"%s\\n\" % d)\n",
        "#     f.write(\"\\n\\n[動物/料理]###############################################################################\\n\")\n",
        "#     for d in s_p:\n",
        "#         f.write(\"%s\\n\" % d)\n",
        "#     f.write(\"\\n\\n[動物/棄却]###############################################################################\\n\")\n",
        "#     for d in s_ab:\n",
        "#         f.write(\"%s\\n\" % d)                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uvzui98ljoY"
      },
      "source": [
        "## **訓練データ作成**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ID8nRVMeWN-"
      },
      "source": [
        "# #テスト分割無し訓練データの生成\n",
        "# import pandas as pd\n",
        "# import glob\n",
        "\n",
        "# path = \"/content/drive/My Drive/Google Colab/niconico_dataset_add_smid/実験\" # use your path\n",
        "# all_files = glob.glob(path + \"/*.csv\")#csv\n",
        "# list_dataset = []\n",
        " \n",
        "# for filename in all_files:\n",
        "#     df2 = pd.read_csv(filename)#csv\n",
        "#     list_dataset.append(df2)\n",
        " \n",
        "# df = pd.concat(list_dataset, axis=0, ignore_index=True)\n",
        "# df = df.sample(frac=1, random_state=45).reset_index(drop=True)\n",
        "\n",
        "# # 分割してtsvファイルで保存する\n",
        "# df.to_csv(\"/content/drive/MyDrive/Google Colab/niconico_dataset_add_smid/TestDate/TRAIN.tsv\", sep='\\t', index=False, header=None)\n",
        "# print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}