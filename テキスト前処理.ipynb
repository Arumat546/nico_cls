{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "テキスト前処理.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1t6-ZgXUviteyVxFXF_VYKcBdr0Vvc9-4",
      "authorship_tag": "ABX9TyNzlLp7oP2Hznb3HTTTKw4+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arumat546/nico_cls/blob/main/%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E5%89%8D%E5%87%A6%E7%90%86.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I48ilNmkO60S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff76d659-cd27-467c-8c7d-3a963486882f"
      },
      "source": [
        "%cd /content/drive/My Drive/Google Colab/niconico_dataset_add_smid"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS57neVSbGEm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "cde02874-856a-421a-b444-964cc5a4214b"
      },
      "source": [
        "#実行\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "df = pd.read_table(\"seizi.tsv\", encoding='utf-8')\n",
        "#print(df)\n",
        "#print(df['content'][8])\n",
        "\n",
        "#前処理\n",
        "df['content'] = df['content'].map(clean_text)\n",
        "df['content'] = df['content'].map(normalize)\n",
        "\n",
        "#3文字以下の文章の除去\n",
        "df['content'] = df['content'].astype('str')\n",
        "mask = (df['content'].str.len() > 3)\n",
        "df = df.loc[mask]\n",
        "print(df)\n",
        "\n",
        "\n",
        "df.to_csv(\"seizi8000.tsv\", sep='\\t', index=None, encoding='utf-8')\n",
        "#print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                        content  category\n",
            "0                           0年以内に日本の風景はガラリと変わるぞ         0\n",
            "1           人は常は無限に足りないので外国人を入れても解消されませんだからいらない         0\n",
            "2                               あべぴょん信者冷えてるかぁ~w         0\n",
            "3                       これで日本は移民大国として世界0位に躍り出るぞ         0\n",
            "4                    その移民が年間0万人の日本人を殺す未来しか見えないな         0\n",
            "...                                         ...       ...\n",
            "38423               康はronに裏切られましたが政治ですから甘すぎませんか         0\n",
            "38424                  弱みを捕まれるような行動を取っていると罠にかかる         0\n",
            "38425  新幹線で東京-新横浜は約0km弱 大体東京-三島が約0kmで東京-掛川が約0km         0\n",
            "38426                               青山繁晴先生と日蓮聖人         0\n",
            "38427                                 三方ヶ原の戦いじゃ         0\n",
            "\n",
            "[38088 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqLPLOkujkgu"
      },
      "source": [
        "## **記号除去**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxFc1kD-graK"
      },
      "source": [
        "#記号除去\n",
        "import string\n",
        "def clean_text(text):\n",
        "    #replaced_text = '\\n'.join(s.strip() for s in text.splitlines()[2:] if s != '')  # skip header by [2:]\n",
        "    replaced_text = text.lower()\n",
        "    replaced_text = re.sub(r'[【】]', '', text)       # 【】の除去\n",
        "    replaced_text = re.sub(r'[（）()]', '', replaced_text)     # （）の除去\n",
        "    replaced_text = re.sub(r'[［］\\[\\]]', '', replaced_text)   # ［］の除去\n",
        "    replaced_text = re.sub(r'[「」｢｣『』]', '', replaced_text)     # 「」の除去\n",
        "    replaced_text = re.sub(r'[{}｛｝]', '', replaced_text)    #  {}の除去\n",
        "    replaced_text = re.sub(r'[；;＾￥：:、。・￥!！\"#$%&=~”＃＄％＆’|*+<>/?？_\\.※‥…→←↑↓]', '', replaced_text)    #  その他記号の除去   \n",
        "    replaced_text = re.sub(r'https?:\\/\\/.*?[\\r\\n ]', '', replaced_text)  # URLの除去\n",
        "    replaced_text = re.sub(r'　', '', replaced_text)  # 全角空白の除去 \n",
        "    return replaced_text    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVsT__VSjqEc"
      },
      "source": [
        "## **基本前処理**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5XWWqJbPFJT"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "#まとめて処理\n",
        "def normalize(text):\n",
        "    normalized_text = normalize_unicode(text)\n",
        "    normalized_text = normalize_number(normalized_text)\n",
        "    normalized_text = lower_text(normalized_text)\n",
        "    normalized_text = reduce_seq(normalized_text)\n",
        "    normalized_text = reduce_w(normalized_text)\n",
        "    return normalized_text\n",
        "\n",
        "\n",
        "#大文字→小文字\n",
        "def lower_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "#半角全角統一\n",
        "def normalize_unicode(text, form='NFKC'):\n",
        "    normalized_text = unicodedata.normalize(form, text)\n",
        "    return normalized_text\n",
        "\n",
        "#数字列の置換\n",
        "def normalize_number(text):\n",
        "    \"\"\"\n",
        "    pattern = r'\\d+'\n",
        "    replacer = re.compile(pattern)\n",
        "    result = replacer.sub('0', text)\n",
        "    \"\"\"\n",
        "    # 連続した数字を0で置換\n",
        "    replaced_text = re.sub(r'\\d+', '0', text)\n",
        "    return replaced_text\n",
        "\n",
        "#3文字以上連続を2文字に変換\n",
        "def reduce_seq(text):\n",
        "\n",
        "  result = re.sub(r'(.)\\1{2,}',r'\\1', text)\n",
        "  return result\n",
        "#二文字の除草\n",
        "def reduce_w(text):\n",
        "\n",
        "  result = re.sub(r'w{2}',r'', text)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc_1NY3ErA7P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "615cacfe-de49-4fb1-ad9d-91f26d59ccef"
      },
      "source": [
        "text='ここはこここはここここはこここここww'\n",
        "ntext=reduce_w(text)\n",
        "print(ntext)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ここはこここはここここはこここここ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKeobPQYjzWD"
      },
      "source": [
        "## **同義語の統一**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-l1wjgCj3rx"
      },
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def lemmatize_term(term, pos=None):\n",
        "    if pos is None:\n",
        "        synsets = wordnet.synsets(term)\n",
        "        if not synsets:\n",
        "            return term\n",
        "        pos = synsets[0].pos()\n",
        "        if pos == wordnet.ADJ_SAT:\n",
        "            pos = wordnet.ADJ\n",
        "    return nltk.WordNetLemmatizer().lemmatize(term, pos=pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7W2TdSZkX6N"
      },
      "source": [
        "## **絵文字と顔文字除去**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FoT-antk9ax"
      },
      "source": [
        "## **ストップワード除去**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVhDCB1DSINj"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "\n",
        "from gensim import corpora\n",
        "\n",
        "\n",
        "def maybe_download(path):\n",
        "    url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
        "    if os.path.exists(path):\n",
        "        print('File already exists.')\n",
        "    else:\n",
        "        print('Downloading...')\n",
        "        # Download the file from `url` and save it locally under `file_name`:\n",
        "        urllib.request.urlretrieve(url, path)\n",
        "\n",
        "\n",
        "def create_dictionary(texts):\n",
        "    dictionary = corpora.Dictionary(texts)\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "def remove_stopwords(words, stopwords):\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "    return words\n",
        "\n",
        "\n",
        "def most_common(docs, n=100):\n",
        "    fdist = Counter()\n",
        "    for doc in docs:\n",
        "        for word in doc:\n",
        "            fdist[word] += 1\n",
        "    common_words = {word for word, freq in fdist.most_common(n)}\n",
        "    print('{}/{}'.format(n, len(fdist)))\n",
        "    return common_words\n",
        "\n",
        "\n",
        "def get_stop_words(docs, n=100, min_freq=1):\n",
        "    fdist = Counter()\n",
        "    for doc in docs:\n",
        "        for word in doc:\n",
        "            fdist[word] += 1\n",
        "    common_words = {word for word, freq in fdist.most_common(n)}\n",
        "    rare_words = {word for word, freq in fdist.items() if freq <= min_freq}\n",
        "    stopwords = common_words.union(rare_words)\n",
        "    print('{}/{}'.format(len(stopwords), len(fdist)))\n",
        "    return stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8bp1Z7tj6vh"
      },
      "source": [
        "## **試行**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQOz9RoxPMo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96290ae2-0159-4240-9038-7799a684eca5"
      },
      "source": [
        "text='私はわたし  であるが1235555toTOsonyは；＾￥#$%&=~|*+<>/?_\\.「」[]{}()（）ｿﾆｰソニーｋk9ああああaaaa'\n",
        "ntext=clean_text(text)\n",
        "print(ntext)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "私はわたし  であるが1235555totosonyは\\ｿﾆｰソニーｋk9ああああaaaa\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}