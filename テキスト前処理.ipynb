{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "テキスト前処理.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1t6-ZgXUviteyVxFXF_VYKcBdr0Vvc9-4",
      "authorship_tag": "ABX9TyM2ei87Xoa0iCwawc0NQCup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arumat546/nico_cls/blob/main/%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E5%89%8D%E5%87%A6%E7%90%86.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I48ilNmkO60S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "263b5ac5-7d0a-4f2e-ab3f-572f4089deb4"
      },
      "source": [
        "%cd /content/drive/My Drive/Google Colab/niconico_dataset_add_smid"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Google Colab/niconico_dataset_add_smid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmdK04WJwJo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac58d05d-f194-49bc-d876-3856dd40a263"
      },
      "source": [
        "# 乱数シードの固定\r\n",
        "import os\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "\r\n",
        "SEED_VALUE = 10  # これはなんでも良い\r\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\r\n",
        "random.seed(SEED_VALUE)\r\n",
        "np.random.seed(SEED_VALUE)\r\n",
        "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f063a032ca8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsiEDlSl3KVx"
      },
      "source": [
        "#tsv出力\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import csv\n",
        "\n",
        "i=0\n",
        "df = pd.DataFrame(index=[], columns=['content','category'])\n",
        "\n",
        "con = sqlite3.connect(\"2525.db\")\n",
        "c = con.cursor()\n",
        "c.execute(\"select content,category from comment5 inner join video on comment5.video_id=video.video_id where length(content)>=8 and category='動物'\")\n",
        "for row in c: # rowはtuple\n",
        "    #print(row[0], row[1])\n",
        "    df.loc[i]=[row[0], row[1]]\n",
        "    i+=1\n",
        "\n",
        "df.head()\n",
        "#データをシャッフル\n",
        "df = df.sample(frac=1, random_state=32).reset_index(drop=True)\n",
        "\n",
        "#データ前処理\n",
        "df['content'] = df['content'].map(clean_text)\n",
        "df['content'] = df['content'].map(normalize)\n",
        "#df['content'] = df['content'].map(labeling)\n",
        "\n",
        "#6文字以下の文章の除去\n",
        "df['content'] = df['content'].astype('str')\n",
        "mask = (df['content'].str.len() >= 7)\n",
        "df = df.loc[mask]\n",
        "#print(df)\n",
        "\n",
        "\n",
        "#取り出すデータ数指定\n",
        "df[:200].to_csv('/content/drive/My Drive/Google Colab/niconico_dataset_add_smid/DATASET/ppp.tsv', sep='\\t', index=False, header=None)\n",
        "#print(df[:len].shape)\n",
        "#df.to_csv('/content/entame.tsv', sep='\\t', index=False)\n",
        "con.commit()\n",
        "con.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS57neVSbGEm"
      },
      "source": [
        "#前処理実行\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "df = pd.read_table(\"seizi.tsv\", encoding='utf-8')\n",
        "#print(df)\n",
        "#print(df['content'][8])\n",
        "\n",
        "#前処理\n",
        "df['content'] = df['content'].map(clean_text)\n",
        "df['content'] = df['content'].map(normalize)\n",
        "df['category'] = df['category'].map(labeling)\n",
        "\n",
        "#4文字以下の文章の除去\n",
        "df['content'] = df['content'].astype('str')\n",
        "mask = (df['content'].str.len() > 4)\n",
        "df = df.loc[mask]\n",
        "print(df)\n",
        "\n",
        "\n",
        "df.to_csv(\"seizi8000.tsv\", sep='\\t', index=None, encoding='utf-8')\n",
        "#print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VxgkyDe1f3r"
      },
      "source": [
        "#ラベルの数値化\n",
        "def labeling(text):\n",
        "    text = re.sub(r'  ゲーム', ' 2', text)\n",
        "    #text = re.sub(r' ', '  1', text)\n",
        "    #text = re.sub(r' ', '  2', text)\n",
        "    return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqLPLOkujkgu"
      },
      "source": [
        "## **記号除去**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxFc1kD-graK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "e76ab44a-40eb-46d0-87d8-4960b5d96b15"
      },
      "source": [
        "#記号除去\n",
        "!pip install neologdn\n",
        "!pip install emoji\n",
        "import string\n",
        "import re\n",
        "import neologdn\n",
        "import emoji\n",
        "def clean_text(text):\n",
        "  text = neologdn.normalize(text)#全角・半角の統一  \n",
        "  text = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '',text)  # URLの除去 \n",
        "  text = re.sub(r'[!-/:-@[-`{-~]', r' ', text)# 半角記号の置換\n",
        "  text = re.sub(u'[■-♯]', ' ', text)#全角記号の置換\n",
        "  text = re.sub(r'[ヽ「」ω仝و∵ξν〃━➑✪█◢†彡ヾ∀∇≦°д゛※‥…→←↑・↓]', '',text)    #  特殊文字の除去\n",
        "  text = re.sub(r'、', ' ',text)#、。の空白化\n",
        "  text = re.sub(r'。', ' ',text)\n",
        "  text = re.sub(r'　', ' ',text)  # 全角空白の半角化\n",
        "  text = ''.join(['' if c in emoji.UNICODE_EMOJI else c for c in text])#絵文字除去\n",
        "  \n",
        "  return text\n",
        "\n",
        "\"\"\"\n",
        "def clean_text(text):\n",
        "    #replaced_text = '\\n'.join(s.strip() for s in text.splitlines()[2:] if s != '')  # skip header by [2:]\n",
        "    replaced_text = text.lower()\n",
        "    replaced_text = re.sub(r'[【】]', '', text)       # 【】の除去\n",
        "    replaced_text = re.sub(r'[（）()]', '', replaced_text)     # （）の除去\n",
        "    replaced_text = re.sub(r'[［］\\[\\]]', '', replaced_text)   # ［］の除去\n",
        "    replaced_text = re.sub(r'[「」｢｣『』]', '', replaced_text)     # 「」の除去\n",
        "    replaced_text = re.sub(r'[{}｛｝]', '', replaced_text)    #  {}の除去\n",
        "    replaced_text = re.sub(r'[；;＾^￥：:、。・￥!！\"#$%&=~”＃＄％＆’|*.※‥…→←↑↓]', '', replaced_text)    #  その他記号の除去   \n",
        "    replaced_text = re.sub(r'https?:\\/\\/.*?[\\r\\n ]', '', replaced_text)  # URLの除去   \n",
        "    replaced_text = re.sub(r'[ヽω仝و∵ξν〃━➑✪█◢†彡ヾ∀∇≦°д゛]', '', replaced_text)    #  特殊文字の除去\n",
        "    replaced_text = re.sub(r'　', '', replaced_text)  # 全角空白の除去\n",
        "\n",
        "    return replaced_text    \n",
        "\"\"\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting neologdn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/fd/9e84b382e4f12b73737faabeeb57fd617198dbb29b7084e28604803f7926/neologdn-0.4.tar.gz (59kB)\n",
            "\r\u001b[K     |█████▌                          | 10kB 18.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20kB 10.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 30kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 40kB 6.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: neologdn\n",
            "  Building wheel for neologdn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neologdn: filename=neologdn-0.4-cp36-cp36m-linux_x86_64.whl size=186915 sha256=42116a5ec595d34c70c2e8c5ddda8f1e63e59defe32f00d6431f29d6d917bd6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/6f/d4/c132b4e7aef22019e307e7673d97010644c9c15f28c0d0b018\n",
            "Successfully built neologdn\n",
            "Installing collected packages: neologdn\n",
            "Successfully installed neologdn-0.4\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49717 sha256=7b70b4014e0e1167fe39f085999c3b9be0b39282aeb03449c4aa0bfb61e446dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef clean_text(text):\\n    #replaced_text = \\'\\n\\'.join(s.strip() for s in text.splitlines()[2:] if s != \\'\\')  # skip header by [2:]\\n    replaced_text = text.lower()\\n    replaced_text = re.sub(r\\'[【】]\\', \\'\\', text)       # 【】の除去\\n    replaced_text = re.sub(r\\'[（）()]\\', \\'\\', replaced_text)     # （）の除去\\n    replaced_text = re.sub(r\\'[［］\\\\[\\\\]]\\', \\'\\', replaced_text)   # ［］の除去\\n    replaced_text = re.sub(r\\'[「」｢｣『』]\\', \\'\\', replaced_text)     # 「」の除去\\n    replaced_text = re.sub(r\\'[{}｛｝]\\', \\'\\', replaced_text)    #  {}の除去\\n    replaced_text = re.sub(r\\'[；;＾^￥：:、。・￥!！\"#$%&=~”＃＄％＆’|*.※‥…→←↑↓]\\', \\'\\', replaced_text)    #  その他記号の除去   \\n    replaced_text = re.sub(r\\'https?:\\\\/\\\\/.*?[\\r\\n ]\\', \\'\\', replaced_text)  # URLの除去   \\n    replaced_text = re.sub(r\\'[ヽω仝و∵ξν〃━➑✪█◢†彡ヾ∀∇≦°д゛]\\', \\'\\', replaced_text)    #  特殊文字の除去\\n    replaced_text = re.sub(r\\'\\u3000\\', \\'\\', replaced_text)  # 全角空白の除去\\n\\n    return replaced_text    \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fpbm5_E1LY_6",
        "outputId": "e833707b-cf69-488a-aa10-17d7f86e7693"
      },
      "source": [
        "text=\"d　a n,c、e。D.A!N！C?E？\"\n",
        "print(clean_text(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "d a n c e D A N C E \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVsT__VSjqEc"
      },
      "source": [
        "## **基本前処理**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5XWWqJbPFJT"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "#まとめて処理\n",
        "def normalize(text):\n",
        "    #normalized_text = normalize_unicode(text)\n",
        "    normalized_text = normalize_number(text)\n",
        "    normalized_text = lower_text(normalized_text)\n",
        "    normalized_text = reduce_seq(normalized_text)\n",
        "    normalized_text = reduce_w(normalized_text)\n",
        "    return normalized_text\n",
        "\n",
        "\n",
        "#大文字→小文字\n",
        "def lower_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "#半角全角統一\n",
        "#def normalize_unicode(text, form='NFKC'):\n",
        " #   normalized_text = unicodedata.normalize(form, text)\n",
        "  #  return normalized_text\n",
        "\n",
        "#数字列の置換\n",
        "def normalize_number(text):\n",
        "    \"\"\"\n",
        "    pattern = r'\\d+'\n",
        "    replacer = re.compile(pattern)\n",
        "    result = replacer.sub('0', text)\n",
        "    \"\"\"\n",
        "    # 連続した数字を0で置換\n",
        "    replaced_text = re.sub(r'\\d+', '0', text)\n",
        "    return replaced_text\n",
        "\n",
        "#3文字以上連続を2文字に変換\n",
        "def reduce_seq(text):\n",
        "\n",
        "  result = re.sub(r'(.)\\1{2,}',r'\\1', text)\n",
        "  return result\n",
        "#二文字の除草\n",
        "def reduce_w(text):\n",
        "\n",
        "  result = re.sub(r'w{2}',r'', text)\n",
        "  return result"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc_1NY3ErA7P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "615cacfe-de49-4fb1-ad9d-91f26d59ccef"
      },
      "source": [
        "text='ここはこここはここここはこここここww'\n",
        "ntext=reduce_w(text)\n",
        "print(ntext)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ここはこここはここここはこここここ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKeobPQYjzWD"
      },
      "source": [
        "## **同義語の統一**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-l1wjgCj3rx"
      },
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def lemmatize_term(term, pos=None):\n",
        "    if pos is None:\n",
        "        synsets = wordnet.synsets(term)\n",
        "        if not synsets:\n",
        "            return term\n",
        "        pos = synsets[0].pos()\n",
        "        if pos == wordnet.ADJ_SAT:\n",
        "            pos = wordnet.ADJ\n",
        "    return nltk.WordNetLemmatizer().lemmatize(term, pos=pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FoT-antk9ax"
      },
      "source": [
        "## **ストップワード除去**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVhDCB1DSINj"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "\n",
        "from gensim import corpora\n",
        "\n",
        "\n",
        "def maybe_download(path):\n",
        "    url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
        "    if os.path.exists(path):\n",
        "        print('File already exists.')\n",
        "    else:\n",
        "        print('Downloading...')\n",
        "        # Download the file from `url` and save it locally under `file_name`:\n",
        "        urllib.request.urlretrieve(url, path)\n",
        "\n",
        "\n",
        "def create_dictionary(texts):\n",
        "    dictionary = corpora.Dictionary(texts)\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "def remove_stopwords(words, stopwords):\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "    return words\n",
        "\n",
        "\n",
        "def most_common(docs, n=100):\n",
        "    fdist = Counter()\n",
        "    for doc in docs:\n",
        "        for word in doc:\n",
        "            fdist[word] += 1\n",
        "    common_words = {word for word, freq in fdist.most_common(n)}\n",
        "    print('{}/{}'.format(n, len(fdist)))\n",
        "    return common_words\n",
        "\n",
        "\n",
        "def get_stop_words(docs, n=100, min_freq=1):\n",
        "    fdist = Counter()\n",
        "    for doc in docs:\n",
        "        for word in doc:\n",
        "            fdist[word] += 1\n",
        "    common_words = {word for word, freq in fdist.most_common(n)}\n",
        "    rare_words = {word for word, freq in fdist.items() if freq <= min_freq}\n",
        "    stopwords = common_words.union(rare_words)\n",
        "    print('{}/{}'.format(len(stopwords), len(fdist)))\n",
        "    return stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8bp1Z7tj6vh"
      },
      "source": [
        "## **コメントAPI**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQOz9RoxPMo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b52c05-3eb9-4b3f-c4d2-3bc82aa73452"
      },
      "source": [
        "#コメント取得API\n",
        "%cd /content/drive/My Drive/Google Colab/niconico_dataset_add_smid/TestDate\n",
        "import os.path\n",
        "import re\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "filepath = 'APIペースト用.txt'\n",
        "extracted_text_array = []\n",
        "pattern_prev = '\">'\n",
        "pattern_next = '</chat>'\n",
        "\n",
        "extracted_text_array = []\n",
        "pattern = pattern_prev + '(.*)' + pattern_next\n",
        "with open(filepath) as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    tmp_extracted_text_array = re.findall(pattern, line)\n",
        "    extracted_text_array.extend(tmp_extracted_text_array)\n",
        "\n",
        "#print(extracted_text_array)\n",
        "\n",
        "i=0\n",
        "df = pd.DataFrame(index=[], columns=['content','category'])\n",
        "for row in extracted_text_array:\n",
        "  df.loc[i]=[row,0]#２つ目の要素はカテゴリ\n",
        "  #print(row)\n",
        "  i+=1\n",
        "\n",
        "df.head()\n",
        "#データをシャッフル\n",
        "#df = df.sample(frac=1, random_state=32).reset_index(drop=True)\n",
        "\n",
        "#データ前処理\n",
        "df['content'] = df['content'].map(clean_text)\n",
        "df['content'] = df['content'].map(normalize)\n",
        "\n",
        "#6文字以下の文章の除去\n",
        "df['content'] = df['content'].astype('str')\n",
        "mask = (df['content'].str.len() >= 7)\n",
        "df = df.loc[mask]\n",
        "\n",
        "df.to_csv('/content/drive/My Drive/Google Colab/niconico_dataset_add_smid/TestDate/君が代.tsv', sep='\\t', index=False, header=None)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Google Colab/niconico_dataset_add_smid/TestDate\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}