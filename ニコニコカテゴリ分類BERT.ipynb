{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ニコニコカテゴリ分類BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1aCt3mJjoUITuqIOtBCDfu0c5Y5ZLFX9V",
      "authorship_tag": "ABX9TyNcYRGGDU4NSJcDX7HsgvNV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arumat546/nico_cls/blob/main/%E3%83%8B%E3%82%B3%E3%83%8B%E3%82%B3%E3%82%AB%E3%83%86%E3%82%B4%E3%83%AA%E5%88%86%E9%A1%9EBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiQFnZTp-gRg",
        "outputId": "1a4ef8e5-9332-4ad1-922f-885be5cc2014"
      },
      "source": [
        "%cd /content/drive/My Drive/Google Colab/niconico_dataset_add_smid/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Google Colab/niconico_dataset_add_smid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORMBmKi4Pj2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "791d927c-e316-45eb-c981-b2bba50c707f"
      },
      "source": [
        "!pip install -q gwpy\n",
        "%cd /content/drive/My Drive/Google Colab/niconico_dataset_add_smid/\n",
        "\n",
        "# 乱数シードの固定\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED_VALUE = 1234  # これはなんでも良い\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Google Colab/niconico_dataset_add_smid\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f43b3d45c00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqrUrv-vbJYS"
      },
      "source": [
        "## **データローダーの作成**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7Sp7E7nTg1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c783dd8b-6459-4a09-d5d0-77f4c0bb1403"
      },
      "source": [
        "#複数のtsvを同一データフレームに格納\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "path = \"/content/drive/My Drive/Google Colab/niconico_dataset_add_smid/実験\" # use your path\n",
        "#all_files = glob.glob(path + \"/*.tsv\")#tsv\n",
        "all_files = glob.glob(path + \"/*.csv\")#csv\n",
        " \n",
        "list_dataset = []\n",
        " \n",
        "for filename in all_files:\n",
        "    #df2 = pd.read_table(filename)#tsv\n",
        "    df2 = pd.read_csv(filename)#csv\n",
        "    list_dataset.append(df2)\n",
        " \n",
        "df = pd.concat(list_dataset, axis=0, ignore_index=True)\n",
        "#print(df.loc[1000, :])\n",
        "\n",
        "#データのシャッフル\n",
        "df = df.sample(frac=1, random_state=45).reset_index(drop=True)\n",
        "df.head()\n",
        "\n",
        "# 分割してtsvファイルで保存する\n",
        "\n",
        "# 全体の2割の文章数\n",
        "len_0_2 = len(df) // 10\n",
        "\n",
        "# 前から2割をテストデータとする\n",
        "df[:len_0_2].to_csv(\"./testcv.tsv\", sep='\\t', index=False, header=None)\n",
        "print(df[:len_0_2].shape)\n",
        "\n",
        "# 前2割からあとを訓練&検証データとする\n",
        "df[len_0_2:].to_csv(\"./traincv.tsv\", sep='\\t', index=False, header=None)\n",
        "print(df[len_0_2:].shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1800, 2)\n",
            "(16200, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au4NCh3PXvuV"
      },
      "source": [
        "## **ここから**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK3gcZBOYVXx"
      },
      "source": [
        "# MeCabとtransformersの用意\n",
        "%%capture\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.996.5\n",
        "!pip install unidic-lite\n",
        "!pip install transformers==3.5.1\n",
        "!pip install fugashi\n",
        "!pip install ipadic\n",
        "import ipadic\n",
        "import torch\n",
        "import torchtext  # torchtextを使用\n",
        "import torch.nn.functional as F\n",
        "from transformers.modeling_bert import BertModel\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM\n",
        "# 日本語BERTの分かち書き用tokenizerです\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOOZSv5BYcwY"
      },
      "source": [
        "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義\n",
        "\n",
        "max_length = 512  # 東北大学_日本語版の最大の単語数（サブワード数）は512\n",
        "\n",
        "def tokenizer_512(input_text):\n",
        "    \"\"\"torchtextのtokenizerとして扱えるように、512単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\n",
        "    return tokenizer.encode(input_text, max_length=512, return_tensors='pt')[0]\n",
        "\n",
        "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_512, use_vocab=False, lower=False, include_lengths=True, batch_first=True, fix_length=max_length, pad_token=0)\n",
        "# 注意：tokenize=tokenizer.encodeと、.encodeをつけます。padding[PAD]のindexが0なので、0を指定します。\n",
        "\n",
        "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# (注釈)：各引数を再確認\n",
        "# sequential: データの長さが可変か？文章は長さがいろいろなのでTrue.ラベルはFalse\n",
        "# tokenize: 文章を読み込んだときに、前処理や単語分割をするための関数を定義\n",
        "# use_vocab：単語をボキャブラリーに追加するかどうか\n",
        "# lower：アルファベットがあったときに小文字に変換するかどうか\n",
        "# include_length: 文章の単語数のデータを保持するか\n",
        "# batch_first：ミニバッチの次元を用意するかどうか\n",
        "# fix_length：全部の文章をfix_lengthと同じ長さになるように、paddingします\n",
        "# init_token, eos_token, pad_token, unk_token：文頭、文末、padding、未知語に対して、どんな単語を与えるかを指定\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3rwnAVaahOF",
        "cellView": "code"
      },
      "source": [
        "%%capture\n",
        "#@title デフォルトのタイトル テキスト\n",
        "# 各tsvファイルを読み込み、分かち書きをしてdatasetにします\n",
        "dataset_train_eval, dataset_test = torchtext.data.TabularDataset.splits(path='.', train='traincv2.tsv', test='testcv2.tsv', format='tsv', fields=[('content', TEXT), ('category', LABEL)])\n",
        "\n",
        "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
        "import random\n",
        "\n",
        "dataset_train, dataset_eval = dataset_train_eval.split(split_ratio=1.0 - 1800/16200, random_state=random.seed(1234))\n",
        "\n",
        "#datasetの長さを確認してみる\n",
        "print(dataset_train.__len__())\n",
        "print(dataset_eval.__len__())\n",
        "print(dataset_test.__len__())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdrN6FTba3ZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef7946e-0462-443b-94c9-e4dd0e8f01da"
      },
      "source": [
        "# datasetの中身を確認してみる\n",
        "item = next(iter(dataset_train))\n",
        "print(item.content)\n",
        "print(\"長さ：\", len(item.content))  # 長さを確認 [CLS]から始まり[SEP]で終わる。512より長いと後ろが切れる\n",
        "print(\"ラベル：\", item.category)\n",
        "\n",
        "# datasetの中身を文章に戻し、確認\n",
        "print(tokenizer.convert_ids_to_tokens(item.content.tolist()))  # 文章\n",
        "#dic_id2cat[int(item.category)]  # id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([    2,  6097, 13097,   687,     7,   139,  4830,  6758,  1058,     3])\n",
            "長さ： 10\n",
            "ラベル： 0\n",
            "['[CLS]', 'ポスト', '増やす', 'だけ', 'に', 'なる', 'かも', 'しれ', 'ん', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvYTbwNta-t4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b4d939-6be6-4c85-a166-6eadaf1c4298"
      },
      "source": [
        "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
        "batch_size = 32  # BERTでは16、32あたりを使用する####################################################################\n",
        "\n",
        "dl_train = torchtext.data.Iterator(\n",
        "    dataset_train, batch_size=batch_size, train=True)\n",
        "\n",
        "dl_eval = torchtext.data.Iterator(\n",
        "    dataset_eval, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "dl_test = torchtext.data.Iterator(\n",
        "    dataset_test, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": dl_train, \"val\": dl_eval}\n",
        "\n",
        "# DataLoaderの動作確認 \n",
        "\n",
        "batch = next(iter(dl_test))\n",
        "print(batch)\n",
        "print(batch.content[0].shape)\n",
        "print(batch.category.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.data.batch.Batch of size 32]\n",
            "\t[.content]:('[torch.LongTensor of size 32x512]', '[torch.LongTensor of size 32]')\n",
            "\t[.category]:[torch.LongTensor of size 32]\n",
            "torch.Size([32, 512])\n",
            "torch.Size([32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kohYRatQyQYZ"
      },
      "source": [
        "## **BERTのクラス分類用のモデルを用意する**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCSrSx6bLeNx"
      },
      "source": [
        "from torch import nn\n",
        "# BERTの日本語学習済みパラメータのモデル\n",
        "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
        "#モデル読み込み\n",
        "\n",
        "#print(model)\n",
        "\n",
        "class BertForNico(nn.Module):\n",
        "    '''BERTモデルに3クラスを判定する部分をつなげたモデル'''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BertForNico, self).__init__()\n",
        "\n",
        "        # BERTモジュール\n",
        "        self.bert = model  # 日本語学習済みのBERTモデル\n",
        "\n",
        "        # headにクラス予測を追加\n",
        "        # 入力はBERTの出力特徴量の次元768、出力は3クラス\n",
        "        self.cls = nn.Linear(in_features=768, out_features=3)###############################################################################################クラス数注意！！！！！！！！！！！！！！！\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.cls.weight, std=0.02)\n",
        "        nn.init.normal_(self.cls.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        '''\n",
        "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
        "        '''\n",
        "\n",
        "        # BERTの基本モデル部分の順伝搬\n",
        "        # 順伝搬させる\n",
        "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
        "\n",
        "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
        "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
        "        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素\n",
        "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n",
        "        output = self.cls(vec_0)  # 全結合層\n",
        "\n",
        "        return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2UXk-6eLonO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0ffb38-bc2b-4ab9-8ac1-9b460e3f2138"
      },
      "source": [
        "# モデル構築\n",
        "net = BertForNico()\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train()\n",
        "\n",
        "print('ネットワーク設定完了')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ネットワーク設定完了\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azAlV-UJLxWB"
      },
      "source": [
        "## **ファインチューニングの設定**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZI8bZN5Lw-z"
      },
      "source": [
        "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
        "\n",
        "# 1. まず全部を、勾配計算Falseにしてしまう\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
        "for param in net.bert.encoder.layer[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "for param in net.cls.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 最適化手法の設定\n",
        "import torch.optim as optim\n",
        "\n",
        "# BERTの元の部分はファインチューニング\n",
        "optimizer = optim.Adam([\n",
        "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
        "    {'params': net.cls.parameters(), 'lr': 1e-4}\n",
        "])\n",
        "\n",
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5vbrZR9L-JC"
      },
      "source": [
        "## **学習の実施**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrIxYgrD0Yi5"
      },
      "source": [
        "# 従来手法"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcFycrM3L78G"
      },
      "source": [
        "\n",
        "# モデルを学習させる関数を作成\n",
        "\n",
        "#正解率記録用リスト\n",
        "EpAc_train = []\n",
        "EpAc_val = []\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.content[0].to(device)  # 文章\n",
        "                labels = batch.category.to(device)  # ラベル\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BERTに入力\n",
        "                    outputs = net(inputs) #outputs=[32行(バッチ数)、2列（クラス分類結果）]\n",
        "\n",
        "                    loss = criterion(outputs, labels)  # 損失を計算\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測  32データ分の数値が大きい方のクラスを一次元配列で返すメソッド\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            acc = (torch.sum(preds == labels.data)\n",
        "                                   ).double()/batch_size\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n",
        "                                iteration, loss.item(),  acc))\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失と正解数の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            #正解率をリストに格納\n",
        "            if phase == 'train':\n",
        "              EpAc_train.append(epoch_acc)\n",
        "            \n",
        "            else :\n",
        "              EpAc_val.append(epoch_acc)\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    \n",
        "    return net\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HwiH0oydSfa"
      },
      "source": [
        "## **提案手法**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KbknCrYsP8N"
      },
      "source": [
        "# 棄却有りのとき\n",
        "\n",
        "#正解率記録用リスト\n",
        "EpAc_train = []\n",
        "EpAc_val = []\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.content[0].to(device)  # 文章\n",
        "                labels = batch.category.to(device)  # ラベル\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BERTに入力\n",
        "                    outputs = net(inputs)\n",
        "   \n",
        "                    # 損失を計算(DG考慮)\n",
        "                    if epoch >= pretrain:\n",
        "                      outputs = F.softmax(outputs,dim=1)\n",
        "\n",
        "                      outputs, reservation = outputs[:,:-1], outputs[:,-1] #[全行、最後以外全列],[全行、最後の列]\n",
        "\n",
        "                    #unsqeeze(1)でonehotベクトル化\n",
        "                    #labelsの要素が1のとこと同じ要素をoutputsから取り出す\n",
        "                      gain = torch.gather(outputs,dim=1,index=labels.unsqueeze(1)).squeeze() \n",
        "\n",
        "                      doubling_rate = (gain.add(reservation.div(reward))).log()\n",
        "           \n",
        "                      loss = -doubling_rate.mean()\n",
        "\n",
        "                    else:\n",
        "                      loss = criterion(outputs[:,:-1],labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測　_は出力の最大値、predsは出力の最大位置（クラス）\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            acc = (torch.sum(preds == labels.data)\n",
        "                                   ).double()/batch_size\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n",
        "                                iteration, loss.item(),  acc))\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失と正解数の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            #正解率をリストに格納\n",
        "            if phase == 'train':\n",
        "              EpAc_train.append(epoch_acc)\n",
        "            \n",
        "            else :\n",
        "              EpAc_val.append(epoch_acc)\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    \n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryn4MvlOMUlB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0a35f7-d522-40e9-fa1e-7980a7c4db86"
      },
      "source": [
        "# 学習・検証\n",
        "num_epochs = 1\n",
        "pretrain = 5\n",
        "reward =  1.2  #棄却度を決めるハイパーパラメータ\n",
        "\n",
        "net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)#10epochで2.8時間"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用デバイス： cuda:0\n",
            "-----start-------\n",
            "イテレーション 10 || Loss: 1.0901 || 10iter. || 本イテレーションの正解率：0.34375\n",
            "イテレーション 20 || Loss: 1.1362 || 10iter. || 本イテレーションの正解率：0.34375\n",
            "イテレーション 30 || Loss: 1.1039 || 10iter. || 本イテレーションの正解率：0.40625\n",
            "イテレーション 40 || Loss: 1.0932 || 10iter. || 本イテレーションの正解率：0.375\n",
            "イテレーション 50 || Loss: 1.0972 || 10iter. || 本イテレーションの正解率：0.34375\n",
            "イテレーション 60 || Loss: 1.0874 || 10iter. || 本イテレーションの正解率：0.40625\n",
            "イテレーション 70 || Loss: 1.0794 || 10iter. || 本イテレーションの正解率：0.375\n",
            "イテレーション 80 || Loss: 1.0590 || 10iter. || 本イテレーションの正解率：0.46875\n",
            "イテレーション 90 || Loss: 1.1411 || 10iter. || 本イテレーションの正解率：0.34375\n",
            "イテレーション 100 || Loss: 1.1107 || 10iter. || 本イテレーションの正解率：0.34375\n",
            "イテレーション 110 || Loss: 1.0410 || 10iter. || 本イテレーションの正解率：0.4375\n",
            "イテレーション 120 || Loss: 1.1266 || 10iter. || 本イテレーションの正解率：0.3125\n",
            "イテレーション 130 || Loss: 1.1062 || 10iter. || 本イテレーションの正解率：0.28125\n",
            "イテレーション 140 || Loss: 1.0240 || 10iter. || 本イテレーションの正解率：0.46875\n",
            "イテレーション 150 || Loss: 1.1005 || 10iter. || 本イテレーションの正解率：0.40625\n",
            "イテレーション 160 || Loss: 1.0660 || 10iter. || 本イテレーションの正解率：0.46875\n",
            "イテレーション 170 || Loss: 0.9563 || 10iter. || 本イテレーションの正解率：0.5625\n",
            "イテレーション 180 || Loss: 1.0086 || 10iter. || 本イテレーションの正解率：0.40625\n",
            "イテレーション 190 || Loss: 1.0277 || 10iter. || 本イテレーションの正解率：0.5\n",
            "イテレーション 200 || Loss: 0.9796 || 10iter. || 本イテレーションの正解率：0.53125\n",
            "イテレーション 210 || Loss: 1.0328 || 10iter. || 本イテレーションの正解率：0.46875\n",
            "イテレーション 220 || Loss: 1.2610 || 10iter. || 本イテレーションの正解率：0.25\n",
            "イテレーション 230 || Loss: 0.9911 || 10iter. || 本イテレーションの正解率：0.375\n",
            "イテレーション 240 || Loss: 0.8913 || 10iter. || 本イテレーションの正解率：0.46875\n",
            "イテレーション 250 || Loss: 0.9927 || 10iter. || 本イテレーションの正解率：0.40625\n",
            "イテレーション 260 || Loss: 0.8793 || 10iter. || 本イテレーションの正解率：0.4375\n",
            "イテレーション 270 || Loss: 0.7441 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 280 || Loss: 0.8457 || 10iter. || 本イテレーションの正解率：0.53125\n",
            "イテレーション 290 || Loss: 0.8343 || 10iter. || 本イテレーションの正解率：0.53125\n",
            "イテレーション 300 || Loss: 0.9587 || 10iter. || 本イテレーションの正解率：0.59375\n",
            "イテレーション 310 || Loss: 0.7083 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 320 || Loss: 0.7924 || 10iter. || 本イテレーションの正解率：0.5625\n",
            "イテレーション 330 || Loss: 0.6585 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 340 || Loss: 0.8588 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 350 || Loss: 0.7433 || 10iter. || 本イテレーションの正解率：0.5625\n",
            "イテレーション 360 || Loss: 0.7412 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 370 || Loss: 0.7698 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 380 || Loss: 0.8969 || 10iter. || 本イテレーションの正解率：0.59375\n",
            "イテレーション 390 || Loss: 0.7706 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 400 || Loss: 0.6347 || 10iter. || 本イテレーションの正解率：0.75\n",
            "Epoch 1/1 | train |  Loss: 0.9742 Acc: 0.4770\n",
            "Epoch 1/1 |  val  |  Loss: 0.6702 Acc: 0.7163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M4LVn3m1YQN"
      },
      "source": [
        "#モデルを保存\r\n",
        "model_path = 'model_scp.pth'\r\n",
        "torch.save(net_trained.state_dict(), model_path)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kArBQxn6UQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e450884-c9c0-43c8-96cd-cb23eab1caa8"
      },
      "source": [
        "net_trained.load_state_dict(torch.load(\"model_scp.pth\"))#保存済みモデルを使う場合"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFhXPok4Rpva"
      },
      "source": [
        "import re\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_diDebHRNMi"
      },
      "source": [
        "#グラフを描画\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "\n",
        "epoch = [\"1\", \"2\", \"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"]\n",
        "#EpAc_train = [0.60, 0.65, 0.66, 0.70, 0.68]\n",
        "#EpAc_val = [0.63, 0.67, 0.68, 0.72, 0.68]\n",
        "\n",
        "plt.title(\"transition\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.plot(epoch, EpAc_train, label = \"train\")\n",
        "plt.plot(epoch, EpAc_val, label = \"val\")\n",
        "plt.legend(loc = \"upper left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQMthlDopHQx"
      },
      "source": [
        "print(EpAc_train)\r\n",
        "print(EpAc_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1WHtm_1QvRU"
      },
      "source": [
        "## **テスト**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBR8dhIPl0R_"
      },
      "source": [
        "\"\"\"\n",
        "#自作テストデータで検証する場合\n",
        "%%capture\n",
        "dataset_train_eval, dataset_test = torchtext.data.TabularDataset.splits(path='.', train='train.tsv', test='testtt.tsv', format='tsv', fields=[('content', TEXT), ('category', LABEL)])\n",
        "dl_test = torchtext.data.Iterator(dataset_test, batch_size=batch_size, train=False, sort=False)\n",
        "batch = next(iter(dl_test))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ktgt86-Qu4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52efa97f-63eb-4e5f-e365-a24de2115522"
      },
      "source": [
        "\n",
        "#通常のテスト\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "preds_all = np.array([],dtype=int)\n",
        "\n",
        "# テストデータでの正解率を求める\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "# epochの正解数を記録する変数\n",
        "epoch_corrects = 0\n",
        "\n",
        "for batch in tqdm(dl_test):  # testデータのDataLoader\n",
        "    # batchはTextとLableの辞書オブジェクト\n",
        "    # GPUが使えるならGPUにデータを送る\n",
        "    inputs = batch.content[0].to(device)  # 文章\n",
        "    labels = batch.category.to(device)  # ラベル\n",
        "\n",
        "    # 順伝搬（forward）計算\n",
        "    with torch.set_grad_enabled(False):\n",
        "\n",
        "        # BertForNicoに入力\n",
        "        outputs = net_trained(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)  # 損失を計算\n",
        "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "        preds_all = np.append(preds_all, preds.cpu().numpy())\n",
        "       \n",
        "\n",
        "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
        "\n",
        "# 正解率\n",
        "epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
        "\n",
        "print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_acc))\n",
        "#print(preds_all)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 113/113 [02:01<00:00,  1.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "テストデータ3600個での正解率：0.8011\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZOJZqwuiTTc"
      },
      "source": [
        "#提案手法\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "expected_coverage = [99,90,80,70,60]\n",
        "text_data=[]\n",
        "abstention_results = []\n",
        "preds_all = np.array([],dtype=int)\n",
        "\n",
        "# テストデータでの正解率を求める\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "# epochの正解数を記録する変数\n",
        "epoch_corrects = 0\n",
        "\n",
        "for batch in tqdm(dl_test):  # testデータのDataLoader\n",
        "    # batchはTextとLableの辞書オブジェクト\n",
        "    # GPUが使えるならGPUにデータを送る\n",
        "    inputs = batch.content[0].to(device)  # 文章\n",
        "    labels = batch.category.to(device)  # ラベル\n",
        "    #print(batch.content[0])\n",
        "\n",
        "    # 順伝搬（forward）計算\n",
        "    with torch.set_grad_enabled(False):\n",
        "        # BertForNicoに入力\n",
        "      outputs = net_trained(inputs)\n",
        "      values, predictions = torch.max(outputs, 1) \n",
        "      \n",
        "\n",
        "\n",
        "    #損失計算\n",
        "      outputs = F.softmax(outputs,dim=1)\n",
        "      outputs, reservation = outputs[:,:-1], outputs[:,-1] #[全行、最後以外全列],[全行、最後の列]\n",
        "\n",
        "      #abstention_results.extend(zip(list(reservation.cpu().numpy()),list(predictions.eq(labels.data).cpu().numpy(),list(inputs.data.cpu().numpy()))))\n",
        "      abstention_results.extend(zip(list(reservation.cpu().numpy()),list(predictions.eq(labels.data).cpu().numpy()),list(inputs.data.cpu().numpy())))\n",
        "      gain = torch.gather(outputs,dim=1,index=labels.unsqueeze(1)).squeeze()\n",
        "      doubling_rate = (gain.add(reservation.div(reward))).log()\n",
        "\n",
        "      loss = -doubling_rate.mean()\n",
        "      preds_all = np.append(preds_all, predictions.cpu().numpy())\n",
        "      epoch_corrects += torch.sum(predictions == labels.data)  # 正解数の合計を更新\n",
        "        \n",
        "\n",
        "#棄却判定後の計算\n",
        "#棄権の結果を予約の高低に応じて並べ替える(降順)\n",
        "abstention_results.sort(key = lambda x: x[0], reverse=True)#リストの1列目の要素をキーに並び替える\n",
        "\n",
        "# ソートされた結果の「正しいかどうか」のリストを取得する\n",
        "#map(関数、加工元)…加工元データに関数をかけたものをリスト取得する\n",
        "sorted_correct = list(map(lambda x: int(x[1]), abstention_results))\n",
        "\n",
        "#テキストリストを取得\n",
        "for i in range(len(dl_test.dataset)):\n",
        "  text_data.append(abstention_results[i][2])\n",
        "\n",
        "#棄却されたテキストの表示\n",
        "\n",
        "#print(\"sorted_correct\")\n",
        "#print(sorted_correct)\n",
        "\n",
        "size = len(dl_test.dataset)\n",
        "\n",
        "print('accracy of coverage ',end='')\n",
        "for coverage in expected_coverage:\n",
        "  #print('{:.0f}: {:.3f}, '.format(coverage, sum(sorted_correct[int(size*coverage/100):])),end='')\n",
        "  print(coverage)\n",
        "  count1=int(size*(100-coverage)/100) \n",
        "  count2=sum(sorted_correct[count1:])\n",
        "  count3=size-count1 #カバレッジ考慮のデータ数\n",
        "  acc_cov=count2/count3\n",
        "  print(acc_cov)\n",
        "  #if coverage==60:\n",
        "    #for j in range(count1):  \n",
        "      #print(tokenizer.convert_ids_to_tokens(text_data[j]))\n",
        "\n",
        "# 正解率\n",
        "epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
        "print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_acc))\n",
        "\n",
        "#print(preds_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUFvC6K1W_RT"
      },
      "source": [
        "print(sorted_correct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-_c0pcwkhos"
      },
      "source": [
        "finish"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYxHtwAbC_HE"
      },
      "source": [
        "## **推論結果の出力**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypD7rwKBYjv7"
      },
      "source": [
        "#推論結果の出力\n",
        "%cd /content/drive/My Drive/Google Colab/niconico_dataset_add_smid\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "df = pd.Cov = pd.read_csv(\"testCSP2.tsv\", sep='\\t',names=[\"content\", \"category\", \"predict\"])\n",
        "net_trained.eval()  #推論モードに\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    df.at[index, \"predict\"] = preds_all[index]  # GPU環境の場合は「.cpu().numpy()」としてください。\n",
        "\n",
        "df.to_csv(\"predicted_test.tsv\", sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGdLqpEaM69"
      },
      "source": [
        "#混合行列の表示（評価）\n",
        "\n",
        "y_true =[]\n",
        "y_pred =[]\n",
        "df = pd.read_csv(\"predicted_test.tsv\",  sep='\\t',engine=\"python\", encoding=\"utf-8\")\n",
        "for index, row in df.iterrows():\n",
        "    if row['category'] == 0:\n",
        "        y_true.append(\"0\")\n",
        "    if row['category'] ==1:\n",
        "        y_true.append(\"1\")\n",
        "    if row['predict'] ==0:\n",
        "        y_pred.append(\"0\")\n",
        "    if row['predict'] ==1:\n",
        "        y_pred.append(\"1\")\n",
        "    if row['category'] ==2:\n",
        "        y_true.append(\"2\")\n",
        "    if row['predict'] ==2:\n",
        "        y_pred.append(\"2\")\n",
        "\n",
        "print(len(y_true))\n",
        "print(len(y_pred))\n",
        "\n",
        "\n",
        "# 混同行列(confusion matrix)の取得\n",
        "labels = [\"0\", \"1\", \"2\"]\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "# データフレームに変換\n",
        "cm_labeled = pd.DataFrame(cm, columns=labels, index=labels)\n",
        "\n",
        "# 結果の表示\n",
        "cm_labeled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eyDhCVWbtNO"
      },
      "source": [
        "#各種推論計算結果の表示\n",
        "y_true =[]\n",
        "y_pred =[]\n",
        "df = pd.read_csv(\"predicted_test.tsv\", sep='\\t', engine=\"python\", encoding=\"utf-8\")\n",
        "for index, row in df.iterrows():\n",
        "    y_true.append(row[\"category\"])\n",
        "    y_pred.append(row[\"predict\"])\n",
        "\n",
        "print(\"正解率（すべてのサンプルのうち正解したサンプルの割合）={}%\".format((round(accuracy_score(y_true, y_pred),2)) *100 ))\n",
        "print(\"適合率（positiveと予測された中で実際にpositiveだった確率）={}%\".format((round(precision_score(y_true, y_pred),2)) *100 ))\n",
        "print(\"再現率（positiveなデータに対してpositiveと予測された確率）={}%\".format((round(recall_score(y_true, y_pred),2)) *100 ))\n",
        "print(\"F1（適合率と再現率の調和平均）={}%\".format((round(f1_score(y_true, y_pred),2)) *100 ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzp4qSWgbtCc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfWug6jPDFml"
      },
      "source": [
        "## **ハイパーパラメータチューニング**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6qVPcSgDE3V"
      },
      "source": [
        "#訓練＆検証\n",
        "!pip install optuna\n",
        "import optuna\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 9\n",
        "pretrain = 3\n",
        "reward =  1.9  #棄却度を決めるハイパーパラメータ\n",
        "\n",
        "#net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)\n",
        "\n",
        "def objective(trial):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = net.to(device)\n",
        "\n",
        "    #criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Training\n",
        "    for step in range(num_epochs):\n",
        "        train(model, device, train_loader, optimizer, criterion)\n",
        "        \n",
        "    # Evaluation\n",
        "    accuracy = evaluate(model, device, test_loader)\n",
        "  \n",
        "    # 返り値が最小となるようにハイパーパラメータチューニングが実行される\n",
        "    return 1.0 - accuracyoptimizer)(net.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piGmrnMJHp-1"
      },
      "source": [
        "study = optuna.create_study()\n",
        "study.optimize(objective, n_trials=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm0TcooXHqmS"
      },
      "source": [
        "# 一番良い結果\n",
        "study.best_trial\n",
        " \n",
        "'''\n",
        "{'hidden_dim': 72,\n",
        " 'learning_rate': 0.09883,\n",
        " 'n_layers': 4,\n",
        " 'optimizer': 'Adam'}\n",
        "'''\n",
        " \n",
        "# データフレームで確認\n",
        "study.trials_dataframe()\n",
        " \n",
        "# values が objective 関数の返り値\n",
        "study.trials_dataframe().sort_values(\"value\").head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z5_bC5MOdRb"
      },
      "source": [
        "## **その他**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tR4uI3SoMbd"
      },
      "source": [
        "#モデル保存\r\n",
        "model_path = 'model_AGO.pth'\r\n",
        "torch.save(model, model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPWQnpsFoqBX"
      },
      "source": [
        "#モデル読み込み\r\n",
        "model_path = 'modelAGO.pth'\r\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}